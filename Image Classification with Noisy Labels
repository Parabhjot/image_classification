{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Script","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This script includes a PyTorch image classifier that is hyperparameter tuned and includes loss functions resistant to training on data with noisy labels "],"metadata":{"id":"_M3d__ZE3u42"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2lE6F1-BTri"},"outputs":[],"source":["# Import required packages \n","\n","import torch\n","import numpy as np\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler"]},{"cell_type":"code","source":["# Use GPU if available \n","\n","import torch\n","import numpy as np\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"metadata":{"id":"cMZ7Nb09Bc7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get working directory \n","\n","import os\n","\n","os.getcwd()"],"metadata":{"id":"tIYz-19DBfs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset \n","\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import random\n","import numpy as np\n","from PIL import Image\n","import json\n","\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 20\n","\n","def unpickle(file):\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","class cifar_dataset(Dataset):\n","    def __init__(self, dataset, root_dir, transform, mode, noise_file=''):\n","\n","        self.transform = transform\n","        self.mode = mode\n","        self.transition = {0: 0, 2: 0, 4: 7, 7: 7, 1: 1, 9: 1, 3: 5, 5: 3, 6: 6,\n","                           8: 8}  # class transition for asymmetric noise for cifar10\n","        # generate asymmetric noise for cifar100\n","        self.transition_cifar100 = {}\n","        nb_superclasses = 20\n","        nb_subclasses = 5\n","        base = [1, 2, 3, 4, 0]\n","        for i in range(nb_superclasses * nb_subclasses):\n","            self.transition_cifar100[i] = int(base[i % 5] + 5 * int(i / 5))\n","\n","        if self.mode == 'test':\n","            if dataset == 'cifar10':\n","                test_dic = unpickle('%s/test_batch' % root_dir)\n","                self.test_data = test_dic[b'data']\n","                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n","                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n","                self.test_label = test_dic[b'labels']\n","            elif dataset == 'cifar100':\n","                test_dic = unpickle('%s/test' % root_dir)\n","                self.test_data = test_dic[b'data']\n","                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n","                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n","                self.test_label = test_dic[b'fine_labels']\n","        else:\n","            train_data = []\n","            train_label = []\n","            if dataset == 'cifar10':\n","                for n in range(1, 6):\n","                    dpath = '%s/data_batch_%d' % (root_dir, n)\n","                    data_dic = unpickle(dpath)\n","                    train_data.append(data_dic[b'data'])\n","                    train_label = train_label + data_dic[b'labels']\n","                train_data = np.concatenate(train_data)\n","            elif dataset == 'cifar100':\n","                train_dic = unpickle('%s/train' % root_dir)\n","                train_data = train_dic[b'data']\n","                train_label = train_dic[b'fine_labels']\n","                # print(train_label)\n","                # print(len(train_label))\n","            train_data = train_data.reshape((50000, 3, 32, 32))\n","            train_data = train_data.transpose((0, 2, 3, 1))\n","\n","            noise_label = json.load(open(noise_file, \"r\"))\n","\n","            if self.mode == 'train':\n","                self.train_data = train_data\n","                self.noise_label = noise_label\n","                self.clean_label = train_label\n","\n","    def __getitem__(self, index):\n","        if self.mode == 'train':\n","            img, target = self.train_data[index], self.noise_label[index]\n","            img = Image.fromarray(img)\n","            img = self.transform(img)\n","            return img, target, index\n","        elif self.mode == 'test':\n","            img, target = self.test_data[index], self.test_label[index]\n","            img = Image.fromarray(img)\n","            img = self.transform(img)\n","            return img, target\n","\n","    def __len__(self):\n","        if self.mode != 'test':\n","            return len(self.train_data)\n","        else:\n","            return len(self.test_data)\n","\n","class cifar_dataloader():\n","    def __init__(self, dataset, batch_size, num_workers, root_dir, noise_file=''):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.root_dir = root_dir\n","        self.noise_file = noise_file\n","        if self.dataset == 'cifar10':\n","            self.transform_train = transforms.Compose([\n","                transforms.RandomCrop(32, padding=4),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","            ])\n","            self.transform_test = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","            ])\n","        elif self.dataset == 'cifar100':\n","            self.transform_train = transforms.Compose([\n","                transforms.RandomCrop(32, padding=4),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n","            ])\n","            self.transform_test = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n","            ])\n","\n","    def run(self, mode):\n","        if mode == 'train':\n","            train_dataset = cifar_dataset(dataset=self.dataset,\n","                                          root_dir=self.root_dir, transform=self.transform_train, mode=\"train\",\n","                                          noise_file=self.noise_file)\n","            trainloader = DataLoader(\n","                dataset=train_dataset,\n","                batch_size=self.batch_size,\n","                shuffle=True,\n","                num_workers=self.num_workers)\n","            return trainloader, np.asarray(train_dataset.noise_label), np.asarray(train_dataset.clean_label)\n","\n","        elif mode == 'test':\n","            test_dataset = cifar_dataset(dataset=self.dataset,\n","                                         root_dir=self.root_dir, transform=self.transform_test, mode='test')\n","            test_loader = DataLoader(\n","                dataset=test_dataset,\n","                batch_size=self.batch_size,\n","                shuffle=False,\n","                num_workers=self.num_workers)\n","            return test_loader\n","\n","# test the custom loaders for CIFAR\n","dataset = 'cifar10'  # either cifar10 or cifar100\n","data_path = 'C:\\\\Users\\\\liaml\\\\Desktop\\\\cifar-testfield'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n","\n","loader = cifar_dataloader(dataset, \n","                          batch_size=batch_size,\n","                          num_workers=num_workers,\n","                          root_dir=data_path,\n","                          noise_file='%s/cifar10_noisy_labels_task1.json' % (data_path)\n","                          )\n","\n","all_trainloader, noisy_labels, clean_labels = loader.run('train')\n","\n","test_loader = loader.run('test')\n","\n","# specify the image classes\n","classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck']"],"metadata":{"id":"T7nGGgfgBhu0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Network Architecture**"],"metadata":{"id":"jnF7Q0hQBoUG"}},{"cell_type":"code","source":["# Network architecture \n","# Define network \n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","\n","    def __init__(self):\n","        \n","        super(CNN, self).__init__()\n","\n","        self.conv_layer = nn.Sequential(\n","\n","            # Conv Layer block 1\n","            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Conv Layer block 2\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Dropout2d(p=0.05),\n","\n","            # Conv Layer block 3\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","\n","        self.fc_layer = nn.Sequential(\n","            nn.Dropout(p=0.1),\n","            nn.Linear(4096, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.1),\n","            nn.Linear(512, 10)\n","        )\n","\n","\n","    def forward(self, x):\n","        \"\"\"Perform forward.\"\"\"\n","        \n","        # conv layers\n","        x = self.conv_layer(x)\n","        \n","        # flatten\n","        x = x.view(x.size(0), -1)\n","        \n","        # fc layer\n","        x = self.fc_layer(x)\n","\n","        return x\n","\n","# create a complete CNN\n","model = CNN()\n","print(model)\n","\n","# move tensors to GPU if CUDA is available\n","if train_on_gpu:\n","  model.cuda()"],"metadata":{"id":"lAyC-FNcBiab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Select Loss Function and Optimization Function**  \n","Only execute 1 out of the 3 following cells depending on the training data (tasks 1-3) "],"metadata":{"id":"rTnF1C_zBx0g"}},{"cell_type":"code","source":["# Task 1 : Generalized Cross Entropy with q = 0.7\n","\n","import torch.optim as optim\n","\n","class GCELoss(nn.Module):\n","    def __init__(self, num_classes=10, q=0.7):\n","        super(GCELoss, self).__init__()\n","        self.q = q\n","        self.num_classes = num_classes\n","\n","    def forward(self, pred, labels):\n","        pred = F.softmax(pred, dim=1)\n","        pred = torch.clamp(pred, min=1e-10, max=1.0)\n","        label_one_hot = F.one_hot(labels, self.num_classes).float().to(pred.device)\n","        loss = (1. - torch.pow(torch.sum(label_one_hot * pred, dim=1), self.q)) / self.q\n","        return loss.mean()\n","\n","# Criterion for Generalized Cross Entropy\n","criterion = GCELoss(10,0.7)\n","\n","# specify optimizer\n","optimizer = optim.SGD(model.parameters(), lr=.001, momentum=0.9)"],"metadata":{"id":"aBi_GY_X47QE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 2: Generalized Cross Entropy with q = 0.9\n","\n","import torch.optim as optim\n","\n","class GCELoss(nn.Module):\n","    def __init__(self, num_classes=10, q=0.9):\n","        super(GCELoss, self).__init__()\n","        self.q = q\n","        self.num_classes = num_classes\n","\n","    def forward(self, pred, labels):\n","        pred = F.softmax(pred, dim=1)\n","        pred = torch.clamp(pred, min=1e-10, max=1.0)\n","        label_one_hot = F.one_hot(labels, self.num_classes).float().to(pred.device)\n","        loss = (1. - torch.pow(torch.sum(label_one_hot * pred, dim=1), self.q)) / self.q\n","        return loss.mean()\n","\n","# Criterion for Generalized Cross Entropy\n","criterion = GCELoss(10,0.9)\n","\n","# specify optimizer\n","optimizer = optim.SGD(model.parameters(), lr=.001, momentum=0.9) "],"metadata":{"id":"XaflZMCa47TS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Task 3 : Symmetric Cross Entropy\n","\n","import torch.optim as optim\n","\n","class SCELoss(torch.nn.Module):\n","    def __init__(self, alpha, beta, num_classes=10):\n","        super(SCELoss, self).__init__()\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.num_classes = num_classes\n","        self.cross_entropy = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, pred, labels):\n","        # CCE\n","        ce = self.cross_entropy(pred, labels)\n","\n","        # RCE\n","        pred = F.softmax(pred, dim=1)\n","        pred = torch.clamp(pred, min=1e-7, max=1.0)\n","        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n","        label_one_hot = torch.clamp(label_one_hot, min=1e-4, max=1.0)\n","        rce = (-1*torch.sum(pred * torch.log(label_one_hot), dim=1))\n","\n","        # Loss\n","        loss = self.alpha * ce + self.beta * rce.mean()\n","        return loss\n","\n","# Criterion for Symmetric Cross Entropy\n","criterion = SCELoss(0.1,1,10)\n","\n","# specify optimizer\n","optimizer = optim.SGD(model.parameters(), lr=.001, momentum=0.9)"],"metadata":{"id":"MxxcxifB47a7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Model Training**"],"metadata":{"id":"AWlpW70fDXgO"}},{"cell_type":"code","source":["import time\n","\n","time_start = time.time()\n","# Train model \n","\n","# number of epochs to train the model\n","n_epochs = 120\n","#List to store loss to visualize\n","train_losslist = []\n","\n","for epoch in range(1, n_epochs):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    for input, labels, clean_labels in all_trainloader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            input, labels = input.cuda(), labels.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(input)\n","        # calculate the batch loss\n","        loss = criterion(output, labels)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*input.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(all_trainloader.dataset)\n","    train_losslist.append(train_loss)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","        epoch, train_loss))\n","    \n","print('Finished Training')\n","time_stop = time.time()\n","\n","time_taken = time_stop - time_start\n","\n","print(\"Your code took \",time_taken,\" to execute\")"],"metadata":{"id":"Y7XzyIKUCr9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Save Trained model**"],"metadata":{"id":"nSfzCJcKDTvb"}},{"cell_type":"code","source":["torch.save(model.state_dict(), 'model_cifar.pt')"],"metadata":{"id":"ZJy5be5cCxUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Load model**"],"metadata":{"id":"AOz0hPaYDPNe"}},{"cell_type":"code","source":["model.load_state_dict(torch.load('model_cifar.pt'))"],"metadata":{"id":"GNiMhYNZCyh-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Test performance of model on test data set**"],"metadata":{"id":"b9CTNCwEDI-E"}},{"cell_type":"code","source":["\n","\n","correct = 0\n","total = 0\n","# since we're not training, we don't need to calculate the gradients for our outputs\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        # calculate outputs by running images through the network\n","        outputs = model(images)\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n","\n","# See what accuracy is like for each class \n","\n","# prepare to count predictions for each class\n","correct_pred = {classname: 0 for classname in classes}\n","total_pred = {classname: 0 for classname in classes}\n","\n","# again no gradients needed\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        outputs = model(images)\n","        _, predictions = torch.max(outputs, 1)\n","        # collect the correct predictions for each class\n","        for label, prediction in zip(labels, predictions):\n","            if label == prediction:\n","                correct_pred[classes[label]] += 1\n","            total_pred[classes[label]] += 1\n","\n","\n","# print accuracy for each class\n","for classname, correct_count in correct_pred.items():\n","    accuracy = 100 * float(correct_count) / total_pred[classname]\n","    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"],"metadata":{"id":"2rvCSn5JC01e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Example of the results:**"],"metadata":{"id":"z4LjkNjuC-h7"}},{"cell_type":"code","source":["#Accuracy of the network on the 10000 test images: 82 %\n","#Accuracy for class: automobile is 93.0 %\n","#Accuracy for class: bird  is 74.9 %\n","#Accuracy for class: cat   is 66.9 %\n","#Accuracy for class: deer  is 78.2 %\n","#Accuracy for class: dog   is 75.4 %\n","#Accuracy for class: frog  is 86.9 %\n","#Accuracy for class: horse is 88.5 %\n","#Accuracy for class: ship  is 92.9 %\n","#Accuracy for class: truck is 88.1 %"],"metadata":{"id":"RqnpmY8QC6nQ"},"execution_count":null,"outputs":[]}]}